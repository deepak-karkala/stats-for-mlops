---
title: Sharpening the Lens
description: Variance reduction via CUPED and sequential testing
---

## Introduction

DriftCity's experiments team discovered something frustrating: real improvements of 2-3% in revenue were being drowned out by natural variation. Standard A/B tests required running 50,000 samples per variant to detect such subtle effects. Then they learned about **CUPED** (Controlled Experiment Using Pre-Experiment Data) â€” a technique that uses historical data to dramatically reduce variance and detect smaller effects with fewer samples.

<Aside type="info">
**CUPED** is a variance reduction technique that leverages pre-experiment metrics to adjust treatment and control outcomes, reducing noise and increasing statistical power without sacrificing data integrity.
</Aside>

## The Problem: Noisy Metrics

Your ride-share revenue varies significantly day-to-day due to weather, events, and randomness. A true 2% improvement gets lost in this noise. Standard approach: run 100,000 samples. CUPED approach: run 10,000 samples with higher power.

```python
# Example: High variance metric
daily_revenue = [1245, 1189, 1567, 1402, 1198, ...]  # Std Dev: ~$200

# True effect: +$25 per user (2% improvement)
# Without variance reduction: Need 50k samples to detect
# With CUPED: Need only 5k samples to detect
```

<Callout heading="Key Insight">
Variance reduction isn't just faster; it's cheaper, reaches conclusions sooner, and lets you run more experiments in parallel.
</Callout>

### The CUPED Technique

CUPED adjusts experiment outcomes using pre-experiment (baseline) metrics:

**Step 1:** Collect baseline metrics (e.g., revenue per user in week before experiment)

**Step 2:** Calculate adjustment coefficient Î¸:
```
Î¸ = cov(baseline, treatment_outcome) / var(baseline)
```

**Step 3:** Adjust observed outcomes:
```
adjusted_outcome = observed_outcome - Î¸ Ã— (baseline - mean_baseline)
```

```python
# CUPED implementation
import numpy as np
from scipy import stats

# Historical baseline metrics
baseline = [1245, 1189, 1567, 1402, ...]

# Experiment outcomes
treatment_outcome = [1270, 1210, 1595, 1425, ...]
control_outcome = [1245, 1188, 1568, 1402, ...]

# Calculate adjustment coefficient
theta = np.cov(baseline, treatment_outcome)[0, 1] / np.var(baseline)

# Adjust outcomes
adjusted_treatment = treatment_outcome - theta * (baseline - np.mean(baseline))
adjusted_control = control_outcome - theta * (baseline - np.mean(baseline))

# Now compare with much lower variance!
print(f"Variance reduction: {np.var(control_outcome) / np.var(adjusted_control):.1f}x")
```

<Figure caption="CUPED reduces variance by leveraging correlation between baseline and treatment metrics">
  <div style={{
    padding: '40px',
    textAlign: 'center',
    backgroundColor: 'var(--color-bg-secondary)',
    borderRadius: 'var(--radius-md)',
    marginBottom: 'var(--space-4)'
  }}>
    ðŸ“‰ CUPED variance reduction visualization would render here
  </div>
</Figure>

<Aside type="warning">
CUPED only works when baseline metrics are correlated with outcomes. Use it for metrics like daily revenue, not for new user metrics where baseline doesn't exist.
</Aside>

## Sequential Testing

Rather than waiting for a fixed sample size, **sequential testing** allows you to monitor results continuously and stop early when you have sufficient evidence.

**Traditional approach:**
1. Calculate required sample size (n=10,000 per group)
2. Run experiment until reaching n
3. Run statistical test once
4. Declare winner or continue

**Sequential approach:**
1. Check results every day
2. Stop when evidence is overwhelming
3. Rejects null hypothesis with controlled error rates
4. Can stop 30-40% earlier on average

```python
# Sequential test pseudocode
for day in experiment_days:
    control_count = count_users(control_group, day)
    treatment_count = count_users(treatment_group, day)

    # Check stopping rules
    if control_count > 5000:
        z_score = calculate_z_score(control, treatment)

        # Stop if p-value < 0.025 (one-tailed, Bonferroni-corrected)
        if abs(z_score) > 1.96:
            declare_winner()
            break
```

<Aside type="tip">
**Pro Tip:** Sequential testing combined with CUPED is powerful: reduce variance with CUPED, then use sequential tests to stop early. You get both speed and statistical rigor.
</Aside>

## Sample Size Comparison

| Approach | Min Sample Size | Detects 2% Effect? |
|----------|-----------------|-------------------|
| Standard A/B Test | 50,000 per group | âœ“ (80% power) |
| CUPED | 5,000 per group | âœ“ (80% power) |
| Sequential + CUPED | 3,500 per group avg | âœ“ (80% power) |

## Next Steps

Chapter 6 brings everything together: building dashboards and guardrails that automatically monitor all these drift and performance signals in production.

<Callout heading="Summary">
- CUPED dramatically reduces variance using historical baselines
- Variance reduction lets you detect smaller effects faster and cheaper
- Sequential testing allows early stopping with controlled error rates
- Combining both techniques maximizes experiment velocity and rigor
</Callout>

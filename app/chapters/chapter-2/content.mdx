---
title: The Weather Event
description: Covariate shift detection (PSI, KS test, overlay histograms)
---

<Aside tone="story">
  It started with rain. Just weatherâ€”the kind the system had seen before. But this time, DriftCity's
  predictions got soaked. Riders changed their behavior, and the model's inputs no longer matched the world it learned.
</Aside>

<Callout title="What you'll learn">
  - How to detect **covariate drift**â€”changes in input distributions P(X)
  - How to compute **Population Stability Index (PSI)** and **Kolmogorovâ€“Smirnov (KS)** tests
  - How to interpret PSI thresholds and visual patterns of shift
  - Why covariate drift is often a **precursor to concept drift**
</Callout>

## 1. What is Covariate Drift?

Covariate drift occurs when the **distribution of features P(X)** changes, but the underlying relationship between features and target P(Y | X) remains the same â€” for now.
The model's learned patterns still apply, but its inputs no longer represent the world it trained on.

In DriftCity, a sudden rainstorm altered rider behavior:
- Fewer people took short trips (walking became safer than waiting for a ride)
- More people took long trips (public transit was disrupted)
- Surge pricing responded to demand shifts

The _relationships_ between distance and fare still held, but the **input distribution** moved.

## 2. Compare Baseline vs Rainstorm

Below are baseline histograms (blue) overlaid with rainstorm data (amber). Toggle between features to see where the shift is most pronounced.

<HistogramCompare
  id="baseline-rainstorm-compare"
  referenceUrl="/chapters/chapter-2/fixtures/rides_baseline.csv"
  currentUrl="/chapters/chapter-2/fixtures/rides_rainstorm.csv"
  height={360}
  features={[
    { label: "trip_distance_km", value: "trip_distance_km" },
    { label: "surge_multiplier", value: "surge_multiplier" },
    { label: "fare_amount", value: "fare_amount" },
  ]}
  defaultFeature="trip_distance_km"
/>

**Observations:**
- **trip_distance_km:** Rainstorm shows fewer short rides (< 2 km) and a heavier right-tail of long trips (> 10 km)
- **surge_multiplier:** Rain increased surge pricing; higher mean and wider spread
- **fare_amount:** Shifted right due to both distance and surge changes

The model trained on dry-weather patterns will now encounter more extreme inputs â€” and may produce less accurate predictions or intervals.

## 3. Quantifying Shift with PSI and KS

Population Stability Index (PSI) measures the difference between a reference (baseline) distribution and a current distribution. It's widely used in production monitoring because:

- **Non-parametric**: doesn't assume any specific distribution shape
- **Symmetric**: treats both directions of change equally
- **Interpretable**: thresholds are easy to operationalize

<DriftGauge
  id="psi-gauge-ch2"
  referenceUrl="/chapters/chapter-2/fixtures/rides_baseline.csv"
  currentUrl="/chapters/chapter-2/fixtures/rides_rainstorm.csv"
  feature="trip_distance_km"
  thresholds={{ warn: 0.1, alert: 0.25 }}
/>

**PSI Interpretation:**
- **PSI < 0.10:** Stable distribution (âœ… no action)
- **0.10 â‰¤ PSI < 0.25:** Moderate drift (âš ï¸ watch and log)
- **PSI â‰¥ 0.25:** Major shift (ðŸš¨ investigate, possibly retrain)

**Kolmogorovâ€“Smirnov Test** provides a formal statistical hypothesis test:

```python
from scipy.stats import ks_2samp

ks_stat, p_value = ks_2samp(
    baseline["trip_distance_km"],
    rainstorm["trip_distance_km"]
)
# Example result: statistic â‰ˆ 0.36, p < 0.001
# Interpretation: reject H0 (same distribution)
```

Use **KS** for statistical rigor; use **PSI** for operational dashboards with human-friendly thresholds.

## 4. Monitoring Drift Over Time

A single PSI comparison is a snapshot. Real value emerges when you track PSI daily or weekly and watch the trend.

In this example, the rainstorm event (day 10) caused a sudden spike. Without monitoring, the shift might go unnoticed until model performance degrades weeks later.

<DriftGauge
  id="psi-gauge-ch2-today"
  referenceUrl="/chapters/chapter-2/fixtures/rides_baseline.csv"
  currentUrl="/chapters/chapter-2/fixtures/rides_rainstorm.csv"
  feature="surge_multiplier"
  thresholds={{ warn: 0.1, alert: 0.25 }}
/>

A sustained upward drift slope warns that inputs are diverging â€” a signal for:
- **Data engineering:** investigate upstream changes
- **Retraining:** incorporate recent data into model updates
- **Monitoring:** set tighter alerting thresholds

## 5. Run It Yourself

<CodeTabs
  tabs={[
    {
      label: "Python: generate rainstorm dataset",
      language: "python",
      code: `import numpy as np, pandas as pd
rng = np.random.default_rng(9)

# Load baseline
df0 = pd.read_csv("rides_baseline.csv")
N = len(df0)

# Simulate rain: fewer short trips, more long ones
trip = np.clip(rng.normal(7.8, 2.5, N), 0.3, None)
surge = np.clip(rng.lognormal(mean=0.12, sigma=0.20, size=N), 1.0, None)
fare = np.clip(38 + trip*3.5 + rng.normal(0, 6, N), 5, None)

# Create rainstorm dataset
df1 = df0.copy()
df1["trip_distance_km"] = trip
df1["surge_multiplier"] = surge
df1["fare_amount"] = fare
df1.to_csv("rides_rainstorm.csv", index=False)
print("Wrote rides_rainstorm.csv")`
    },
    {
      label: "Python: compute PSI for all features",
      language: "python",
      code: `import pandas as pd, numpy as np
from scipy.stats import ks_2samp

# Load both datasets
baseline = pd.read_csv("rides_baseline.csv")
rainstorm = pd.read_csv("rides_rainstorm.csv")

# Function to compute PSI
def psi(baseline_vals, current_vals, bins=10):
    baseline_vals = baseline_vals.dropna()
    current_vals = current_vals.dropna()

    # Fixed bin edges from baseline quantiles
    qs = np.quantile(baseline_vals, np.linspace(0, 1, bins + 1))
    e = np.histogram(baseline_vals, bins=qs)[0].astype(float)
    a = np.histogram(current_vals, bins=qs)[0].astype(float)

    # Laplace smoothing to avoid zeros
    e = (e + 1e-6) / (e.sum() + bins*1e-6)
    a = (a + 1e-6) / (a.sum() + bins*1e-6)

    return float(np.sum((a - e) * np.log(a / e)))

# Compute PSI for key features
features = ["trip_distance_km", "surge_multiplier", "fare_amount"]
for feat in features:
    psi_val = psi(baseline[feat], rainstorm[feat])
    ks_stat, ks_p = ks_2samp(baseline[feat], rainstorm[feat])
    print(f"{feat:20s} PSI={psi_val:.4f}  KS={ks_stat:.4f} (p={ks_p:.2e})")`
    },
    {
      label: "Node: PSI via Plotly spec",
      language: "ts",
      code: `// Use the same PSI calculation from Chapter 1
// in a component like DriftGauge

// Plotly spec for overlay histogram:
const OverlayHistogramSpec = (ref: number[], cur: number[], feature: string) => ({
  data: [
    {
      type: "histogram" as const,
      x: ref,
      nbinsx: 40,
      name: "Baseline",
      opacity: 0.5,
      marker: { color: "#00D8FF" },
    },
    {
      type: "histogram" as const,
      x: cur,
      nbinsx: 40,
      name: "Rainstorm",
      opacity: 0.5,
      marker: { color: "#FFB347" },
    },
  ],
  layout: {
    barmode: "overlay" as const,
    height: 360,
    margin: { t: 10, r: 10, b: 40, l: 50 },
    xaxis: { title: feature },
    yaxis: { title: "count" },
    legend: { orientation: "h" as const },
  },
  config: { displayModeBar: false, responsive: true },
});`
    }
  ]}
/>

## 6. Real-World Practice: Drift Monitoring in Production

Major tech companies use PSI and KS as part of their model monitoring stacks:

<DataTable
  caption="Industry drift monitoring implementations"
  columns={[
    { key: "company", label: "Company" },
    { key: "implementation", label: "Implementation" },
    { key: "insight", label: "Key Insight" },
  ]}
  rows={[
    {
      company: "Uber Michelangelo",
      implementation: "Nightly feature monitoring jobs compute PSI/KS for all continuous features",
      insight: "Automate drift alerts before training pipelines run",
    },
    {
      company: "DoorDash",
      implementation: "Streaming feature store includes drift detectors with 7-day moving PSI average",
      insight: "Smooth short-term noise; alert only on sustained shifts",
    },
    {
      company: "Pinterest Ads",
      implementation: "Combine PSI with volume metrics to catch missing-data drift",
      insight: "Detect upstream ETL failures that change feature sparsity",
    },
    {
      company: "Airbnb",
      implementation: "Per-segment drift tracking (by geography, user cohort)",
      insight: "Understand where and whom shift affects most",
    },
  ]}
/>

**Common thresholds:**
- **PSI < 0.1** â†’ Log and proceed
- **0.1 â‰¤ PSI < 0.25** â†’ Create ticket, annotate in experiment logs
- **PSI â‰¥ 0.25** â†’ Page on-call engineer, trigger retraining job

## 7. Key Takeaways

<Callout title="Covariate Drift Checklist">
  - Establish **reference histograms** per feature before deployment
  - Compute **PSI & KS** regularly (daily or weekly) between live and baseline windows
  - Alert if **PSI â‰¥ 0.25** or **KS p < 0.01**
  - Track drift **trends** â€” one spike can be noise; sustained growth = real issue
  - Covariate drift is often a **precursor to concept drift** â†’ monitor closely
  - Use **overlay histograms** to identify _where_ the shift is (center vs tails)
</Callout>

## 8. Where This Connects

This chapter showed that **inputs changed** but model performance (in theory) stayed the same. In **Chapter 3: The Vanishing Commuter**, we'll see what happens when the _relationship_ P(Y | X) itself changes â€” and model errors surge despite stable inputs.

<Aside tone="next">
  Next â†’ **Chapter 3: The Vanishing Commuter** â€” when the underlying relationship between features and target shifts, and accuracy plummets.
</Aside>

---
title: When Rain Fell Harder
description: Covariate drift detection & environmental impact on model performance
---

## Introduction

In DriftCity, the rainy season arrived unexpectedly. While the relationship between weather patterns and ride demand remained the same, the underlying distribution of features shifted dramatically. This is **covariate drift** â€” when input features change but the model's learned relationship stays constant.

<Aside type="info">
**Covariate Drift** occurs when the distribution of input features changes between training and production, but the conditional distribution of labels given features remains the same.
</Aside>

## The Problem: Seasonal Changes

Your ride-share model was trained on summer data with consistent temperature and precipitation patterns. When monsoon season arrives, temperature ranges shift, rainfall increases, and user behavior adapts. The model's core logic still applies, but the input distribution has fundamentally changed.

```python
# Example: Feature distribution shift during rainy season
import numpy as np

# Summer baseline
summer_rainfall = np.random.normal(loc=2, scale=0.5, size=1000)

# Monsoon season
monsoon_rainfall = np.random.normal(loc=8, scale=2, size=1000)

print(f"Summer mean: {summer_rainfall.mean():.2f}")
print(f"Monsoon mean: {monsoon_rainfall.mean():.2f}")
```

<Callout heading="Key Insight">
Covariate drift is particularly dangerous because it's often undetected by traditional performance metrics if the relationship between features and labels remains stable.
</Callout>

### Detecting Covariate Drift Over Time

Unlike point-in-time checks, monitoring drift trends reveals patterns:
- Gradual seasonal shifts
- Sudden environmental events
- Long-term demographic changes

<Aside type="warning">
Monitoring a single drift metric at one point in time can mask systematic drift patterns. Always track drift trends over time!
</Aside>

## Statistical Measures: PSI Trends

**Population Stability Index** becomes powerful when tracked continuously. By computing PSI between rolling windows and a baseline, you can detect:

1. **Gradual drift**: PSI slowly increasing over weeks
2. **Sudden shift**: PSI spike after an event
3. **Cyclical patterns**: PSI peaks at predictable times (seasonal)

**Interpretation guidelines:**
- PSI < 0.1: Distribution stable
- PSI 0.1-0.25: Noticeable drift, investigate causes
- PSI > 0.25: Significant drift, consider model retraining

<Figure caption="PSI trend over time reveals drift patterns and seasonal cycles">
  <div style={{
    padding: '40px',
    textAlign: 'center',
    backgroundColor: 'var(--color-bg-secondary)',
    borderRadius: 'var(--radius-md)',
    marginBottom: 'var(--space-4)'
  }}>
    ðŸ“ˆ PSI trend visualization would render here
  </div>
</Figure>

## Understanding Feature Distributions

When comparing distributions directly, use histograms and density plots:

```python
# Compare distributions visually
import matplotlib.pyplot as plt

plt.hist(summer_rainfall, bins=30, alpha=0.5, label='Summer')
plt.hist(monsoon_rainfall, bins=30, alpha=0.5, label='Monsoon')
plt.xlabel('Daily Rainfall (inches)')
plt.ylabel('Frequency')
plt.legend()
plt.show()
```

<Aside type="tip">
**Pro Tip:** For multimodal distributions, histogram comparison gives intuitive insight. For continuous variables, consider quantile-quantile plots to detect tail differences.
</Aside>

## Next Steps

Chapter 3 explores what happens when the relationship between features and labels themselves changes â€” true **concept drift**.

<Callout heading="Summary">
- Covariate drift shifts input distributions while relationships remain stable
- PSI tracking over time reveals drift patterns and seasonality
- Histogram comparison provides visual confirmation of distribution shifts
- Environmental and seasonal events are common drift causes
</Callout>
